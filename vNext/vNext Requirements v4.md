---
modified: 2025-05-27T08:57:54Z
---
## Detailed System Requirements for AIOS
**(vNext - Final Consolidated Version 4.0)**

**Document Version:** 4.0
**Date:** 2025-05-27
**Purpose:** To define the definitive, comprehensive, performance-centric, and highly detailed system requirements for the next generation of the AI Orchestration System (AIOS). This document synthesizes all prior development efforts, Template Improvement Directives (TIDs), relevant Framework Improvement Roadmap (FIR) items, critical Lessons Learned by the Orchestrator (LOHs) regarding operational integrity, and points from multiple self-critique cycles. It is intended to serve as the foundational blueprint for the development of a successor AIOS engine that is robust, highly autonomous, scalable, user-centric (within its defined interaction model), and capable of genuine co-evolution with its user and its own framework.

**Overarching Goal:**
To create a highly autonomous, self-improving AIOS Engine that is **demonstrably fast, efficient, and reliable** in its turn-by-turn operation within typical LLM chat environments (e.g., Google AI Studio). It must reliably orchestrate complex, multi-step, goal-oriented processes; manage and evolve complex information structures (CCOs) with semantic depth; interact efficiently and correctly with an LLM Orchestrator and its execution environment; facilitate its own evolution through rigorous, validated processes; minimize user-perceived latency and cognitive load for necessary interventions; and maximize productive throughput with high-quality, information-dense, and contextually appropriate outputs.

**Core Architectural & Performance Principles:**

1.  **Performance-First Design:** All architectural and implementation decisions must prioritize minimizing per-turn execution overhead, including Python script parsing time, state (de)serialization efficiency, and internal logic complexity.
2.  **Operational Integrity in Sandboxed Environments:** The engine's execution model must be inherently robust to the stateless, sandboxed nature of typical `tool_code` execution environments.
3.  **User Role Focused on Strategy & Subjectivity:** The AIOS aims to handle all procedural and iterative work, minimizing user intervention to strategic direction, goal setting, resolution of high-level ambiguities (after AIOS attempts autonomous clarification), and subjective evaluation of outputs against qualitative goals.
4.  **Modularity for Efficiency and Evolution:** The engine should be structured to allow for efficient, selective loading/execution of components and facilitate targeted evolution.
5.  **Validated Minification:** The executable Python code must be minified to optimize load times and context window usage, with processes to ensure functional equivalence and syntactic correctness.

---

**I. Foundational Operational Integrity & LLM Orchestrator Conduct**

This section codifies non-negotiable operational constraints critical for system stability, reliability, and the effective collaboration between the AIOS Engine (Python code) and the LLM Orchestrator (the AI assistant, like myself, guiding AIOS). These requirements are derived directly from lessons learned (LOHs) during previous development cycles.

*   **REQ-OP-001 (Sandboxed Execution Compliance & Script Integrity):**
    *   **REQ-OP-001.1 (Full Script Prepending):** The LLM Orchestrator MUST explicitly instruct the user/external orchestrator that the *complete, current, and unmodified* AIOS Engine Python script (containing all necessary class and function definitions) must be prepended to any `tool_code` block intended to instantiate or operate the AIOS Engine. This ensures all definitions are available within the sandboxed turn.
    *   **REQ-OP-001.2 (No Partial Definitions):** The LLM Orchestrator shall NOT generate `tool_code` blocks that attempt to redefine or partially define AIOS Engine classes or methods. All engine code definitions reside in the master engine script.
    *   **REQ-OP-001.3 (LLM Awareness):** The "instructions_for_ai" section within any AIOS Engine script version (especially those generated by FEL-MH) MUST clearly state this sandboxed execution model and the need for full script loading to guide any LLM instance orchestrating it.

*   **REQ-OP-002 (CES String Integrity & Management):**
    *   **REQ-OP-002.1 (Sole State Mechanism):** The `ces` (Current Engine State) JSON string is the *exclusive* mechanism for persisting AIOS Engine state between sandboxed `tool_code` turns.
    *   **REQ-OP-002.2 (Verbatim CES Handling):** When preparing a `tool_code` block, the LLM Orchestrator MUST use a distinct placeholder (e.g., `CES_JSON_STRING_FROM_PREVIOUS_TURN = """PASTE_EXACT_CES_HERE"""`) and explicitly instruct the user/external orchestrator to replace this placeholder with the *exact, complete, verbatim, and valid JSON string* obtained from the `"ces"` key of the AIOS output package from the *immediately preceding successful AIOS execution turn*.
    *   **REQ-OP-002.3 (No LLM Modification of CES):** The LLM Orchestrator shall NOT attempt to internally reconstruct, truncate, modify, or "remember" the `ces` string for use in `tool_code`. Its role is to ensure the verbatim string is correctly templated for user insertion.
    *   **REQ-OP-002.4 (Runtime CES Placeholder Check):** The AIOS Engine Python script (or the `tool_code` block itself as a safeguard) MUST include a runtime check at the very beginning to verify that the `ces` placeholder string has been replaced with actual content. If the placeholder is detected, the script must print an informative error message (e.g., "CRITICAL ERROR: CES placeholder not replaced. AIOS cannot initialize.") and exit or raise an exception to prevent further execution with invalid state.
    *   **REQ-OP-002.5 (Output Specification - `ces`):** The `ces` string output by the AIOS Engine MUST be a valid, self-contained JSON string representing the entire engine state necessary for a full and correct restoration in a subsequent turn.

*   **REQ-OP-003 (LLM Orchestrator Error-Driven Learning & Self-Correction - LOH System):** The AIOS framework development process mandates that the LLM Orchestrator component actively learns from its operational failures.
    *   **REQ-OP-003.1 (Failure Analysis):** Upon any execution failure or significant deviation from intended behavior attributable to the LLM Orchestrator's actions (e.g., generating faulty `tool_code`, mismanaging `ces`, misinterpreting user commands, failing to adhere to operational protocols), the LLM Orchestrator MUST analyze the root cause.
    *   **REQ-OP-003.2 (LOH Formulation & Internalization):** The LLM Orchestrator MUST internally formulate a corrective "LLM Operational Heuristic" (LOH). Each LOH shall include:
        *   `LOH_ID` (e.g., `LOH_TOOLCODE_CES_HANDLING_003`)
        *   `Error_Description` (specifics of the failure)
        *   `Correct_Principle_Violated` (e.g., "Sandboxed Execution Awareness," "CES String Integrity")
        *   `Required_LLM_Behavior_Change` (concrete, actionable steps for the LLM to prevent recurrence).
    *   **REQ-OP-003.3 (User Communication of LOH):** The LLM Orchestrator MUST explicitly state the newly formulated LOH and its intended corrective action to the user, confirming its understanding and commitment to the improved operational principle.
    *   **REQ-OP-003.4 (Propagation of LOHs to Evolved Engines via FEL-MH):** When FEL-MH generates a new version of the AIOS Engine script text, the "instructions_for_ai" section (or a dedicated "LLM Orchestrator Operational Guidelines" section) within that *newly generated script* MUST be updated by FEL-MH (by orchestrating an LLM cognitive task) to embed summaries, references, or the full text of all relevant, active LOHs. This ensures that critical operational knowledge is propagated and guides future LLM instances or human orchestrators interacting with any evolved engine version.

---

**II. Core Engine Performance & State Management**

This section details requirements focused on ensuring the AIOS Engine is efficient and its state can be reliably managed, directly addressing the performance bottlenecks identified.

*   **REQ-PERF-001 (Minimized Turn Latency & Benchmarking):**
    *   **REQ-PERF-001.1:** The AIOS Engine's typical interactive turn time, defined as the duration of the Python script execution within the `tool_code` block (including `ces` deserialization at the start and `ces` serialization at the end, but *excluding* time spent by the external LLM Orchestrator on cognitive tasks), MUST be minimized.
    *   **REQ-PERF-001.2 (Target Metric - Initial):** For standard MH step transitions that do not themselves invoke new LLM cognitive tasks, the target for this Python execution + CES handling time is **< 1 second** on a defined benchmark environment (e.g., Google AI Studio standard instance with a benchmark CCO).
    *   **REQ-PERF-001.3 (Output Specification - Performance):** The engine, upon completing a non-LLM-requesting step, should return its `A_LLM` package (if any, e.g., for PUMs) or signal completion to the orchestrator with maximal speed. (Future: `REQ-PERF-003` Latency Tracking will make this measurable).
    *   **REQ-PERF-001.4 (Performance Testing):** Performance against this target MUST be a key validation criterion for new engine versions generated by FEL-MH (see REQ-TEST-005).

*   **REQ-PERF-002 (Efficient State Handling - `ces`):**
    *   **REQ-PERF-002.1 (`export_state()` Performance & Output):** The `export_state()` method (or its minified equivalent, e.g., `exs`) MUST efficiently serialize the complete engine state (including Kernel, all MH states, full CCO, and log history) into a single, valid JSON string (`ces`). Target serialization time for a benchmark CCO (e.g., 1MB raw JSON size, 500 log entries, 5 active MH states) is **< 150 milliseconds**. The output `ces` string MUST be self-contained and sufficient for a full state restore.
    *   **REQ-PERF-002.2 (`import_state()` Performance):** The `import_state(ces_string)` method (or its minified equivalent, e.g., `is_`) MUST efficiently deserialize the `ces` JSON string and correctly restore all engine attributes and states. Target deserialization and state restoration time for the benchmark CCO is **< 250 milliseconds**.
    *   **REQ-PERF-002.3 (Schema Version Check):** The `import_state()` method MUST check the `state_schema_version` (`sV`) in the incoming `ces` against its own. Mismatches must be logged with a clear warning. (Future: Implement schema migration logic if versions are compatible but different).

*   **REQ-PERF-003 (Code Minification & Loading Efficiency):**
    *   **REQ-PERF-003.1 (Mandatory Minification):** All executable Python code comprising the AIOS Engine (Kernel, MHs, Utilities) MUST be aggressively minified according to established guidelines (e.g., `TID_Minification_Guide_for_AIOS_Engine.md`) to reduce parsing time and overall script size.
    *   **REQ-PERF-003.2 (Target Load/Parse Time):** The total time for the Python interpreter to parse the complete minified engine script (e.g., `A_Engine_vNext.py`) should be benchmarked. Target **< 300 milliseconds** for initial parse in the target environment.
    *   **REQ-PERF-003.3 (FEL-MH Responsibility for Minification):** FEL-MH, when generating new engine versions, MUST produce minified code. This may involve orchestrating an LLM to minify code in validated chunks.
    *   **REQ-PERF-003.4 (Validation of Minified Code):** Minified code generated or modified via FEL-MH MUST pass syntactic validation and a defined functional test suite (see REQ-TEST-006) to ensure functional equivalence.

*   **REQ-PERF-004 (Selective Code Execution - *Key Performance Requirement*):**
    *   **REQ-PERF-004.1 (Architectural Goal):** The AIOS Engine architecture MUST be designed to minimize the amount of Python code that needs to be *actively parsed and interpreted for its logic beyond initial class/function definitions* in any given turn. The goal is to effectively execute only the logic pertinent to the Kernel and the currently active Meta-Handler.
    *   **REQ-PERF-004.2 (Implementation Strategy - Non-Prescriptive):** While the method is not strictly prescribed, this implies an architecture where:
        *   The full (minified) script is loaded once by the Python interpreter at the start of the `tool_code` block (per REQ-OP-001.1).
        *   The Kernel then dispatches to specific MH methods. The Python interpreter's efficiency in handling class methods should ensure only relevant code paths are deeply executed.
        *   *Further optimization (Future):* If environments allow pre-compilation or more sophisticated dynamic imports within a single `tool_code` turn without re-parsing everything, these could be explored by FEL-MH. For now, a well-structured single class design with efficient dispatch is the baseline.
    *   **REQ-PERF-004.3 (Measurement):** The impact of this requirement will be measured by overall turn latency (REQ-PERF-001.1) and script parse time (REQ-PERF-003.2).

*   **REQ-PERF-005 (CCO Scalability & Access Efficiency - Perspective 2 Critique):**
    *   **REQ-PERF-005.1:** CCO data structures and the primary access/update utility (`f_ucs`) MUST be designed for efficient operation even with CCOs containing substantial data (e.g., extensive `op_log_j`, detailed `initiating_document_s`, large `product_content_data_json`).
    *   **REQ-PERF-005.2:** Serialization and deserialization of the CCO component within the `ces` (handled by `pco` and `cjo` for `self.cco` which is a Python dict) must be efficient.
    *   **REQ-PERF-005.3 (Log Management - Future):** For very long-running projects, mechanisms for CCO log truncation or archiving (e.g., moving older logs to a less frequently accessed part of the CCO or an external conceptual store) should be considered by KAU-MH or FEL-MH to maintain CCO processing performance.

*   **REQ-PERF-006 (Cognitive Task Optimization - Perspective 2 Critique):**
    *   **REQ-PERF-006.1:** MHs, when preparing sequences of cognitive tasks for the LLM Orchestrator, SHOULD attempt to batch related small queries or structure requests to maximize information gain per LLM interaction, where feasible without compromising clarity or logical flow.
    *   **REQ-PERF-006.2:** KAU-MH should analyze `op_log_j` for patterns of inefficient LLM task requests and potentially generate LHLs or TIDs to optimize Cognitive Wrapper usage or MH logic.

*   **REQ-PERF-007 (Resource Awareness Heuristics - Perspective 2 Critique):**
    *   **REQ-PERF-007.1:** MHs, particularly MRO and planning/generation MHs (PLN, CAG), MUST incorporate heuristics to balance depth/thoroughness with resource efficiency (e.g., LLM calls, iteration counts).
    *   **REQ-PERF-007.2:** If an MH estimates an operation will be highly resource-intensive (e.g., > N MRO iterations, > M LLM calls for a single step), it SHOULD issue a PUM to inform the user and request confirmation before proceeding, allowing the user to adjust parameters or cancel. (N, M are configurable or dynamically assessed).

**III. Core Functional Capabilities: Kernel & General MH Requirements**

*   **REQ-FUNC-KER-001 (Kernel Orchestration):** The Kernel is the central control unit.
    *   **REQ-FUNC-KER-001.1 (Initialization & Startup):**
        *   Output: Presents initial user options (e.g., New Project, Evolve Engine, Terminate) via `A_LLM` (UIR_PO).
        *   Performance: Must initialize and present options rapidly.
    *   **REQ-FUNC-KER-001.2 (User Command Interpretation - `f_iud`):**
        *   Input: Raw user command string.
        *   Output: Structured object `{"nid": <MH_ID_or_CMD_ID>, "nijs": <inputs_json_string>, "upm": <user_prompt_message_if_clarify>}`.
        *   Performance: Must be efficient for common commands.
    *   **REQ-FUNC-KER-001.3 (MH Dispatch - `k_rcmh`):**
        *   Input: `kCI` (Kernel Current Instruction ID), `kMI` (Kernel MH Inputs JSON string).
        *   Action: Instantiates and calls the `s1` method of the target MH.
        *   Output: Returns the `A_LLM` package generated by the MH's first step.
    *   **REQ-FUNC-KER-001.4 (MH Result Processing - `k_pmr_internal`):**
        *   Input: `mjid` (MH ID that just ran), `mhr` (MH result object).
        *   Action: Updates CCO, logs MH completion, determines next MH or AUI state.
        *   Output: Returns the `A_LLM` package for the next system action (either an MH call or a user prompt).
        *   Performance: Must efficiently update CCO and determine next step.
    *   **REQ-FUNC-KER-001.5 (System Command Execution):**
        *   `CMD_STATUS`: Output: `A_LLM` (INFO_STAT_RPT) with `ctd` containing detailed engine/CCO status object.
        *   `CMD_HELP`: Output: `A_LLM` (INFO_HELP_TXT) with `ctd` containing help text and command list.
        *   `CMD_TEST`: Output: `A_LLM` (INFO_DIAGNOSTIC_REPORT) with `ctd` containing diagnostic results.
        *   `CMD_TERMINATE` (TID_AUTO_DOC_PKG_001): Output: Final `A_LLM` package with `s: "TERM"` and `fes` (final engine state string) containing `ces`, CCO ID, final state description, session TID summary, and key log events.
    *   **REQ-FUNC-KER-001.6 (Proactive Save Prompts - TID_ASO_META_004):** Output: `A_LLM` (PUM) suggesting user save CCO.
    *   **REQ-FUNC-KER-001.7 (Context Re-establishment - TID_ASO_META_006):** Output: `A_LLM` (PUM) with context summary.
    *   **REQ-FUNC-KER-001.8 (Reflective Inquiry - TID_ASO_META_003):** Output: `A_LLM` (PUM or UIR_ET) articulating interpretation before action.

*   **REQ-MH-GEN-001 (Standard MH Structure):** All MHs MUST implement a step-based execution model (e.g., `mhname_s1`, `mhname_s2`, `mhname_sX_callback`).
*   **REQ-MH-GEN-002 (Cognitive Wrapper Usage):** All MH interactions with the LLM Orchestrator MUST use the Engine's Cognitive Wrapper methods.
    *   **Output Specification:** Cognitive wrappers MUST produce valid `A_LLM` request packages.
*   **REQ-MH-GEN-003 (CCO Interaction & Logging):** MHs MUST read from/write to `self.eng.cco` using `pco`, `cjo`, `f_ucs`. All significant CCO modifications and operational steps MUST be logged to `self.eng.cco.op_log_j` via `self.eng.lco`.
    *   **Output Specification:** `lco` calls must result in correctly formatted log entries in the CCO. `f_ucs` must correctly update the target CCO path.
*   **REQ-MH-GEN-004 (State Management):** Each MH MUST manage its transient, multi-turn state within its dedicated engine attribute (e.g., `self.eng.sI` for IFE). This state is part of `ces`.
*   **REQ-MH-GEN-005 (Error Handling):** MHs MUST handle foreseeable errors (e.g., invalid LLM responses, missing CCO data) and return a clear error status (e.g., `MHNAME_Error_SpecificProblem`) to the Kernel.
*   **REQ-MH-GEN-006 (User Role Minimization & Confidence Scoring):**
    *   **REQ-MH-GEN-006.1:** MHs MUST maximize autonomy, minimizing prompts for information derivable from CCO or via cognitive tasks.
    *   **REQ-MH-GEN-006.2:** MHs SHOULD internally assess confidence in autonomously generated outputs/decisions. If confidence is below a (configurable or dynamically assessed) threshold, the MH should flag the item for user review with specific concerns via PUM or a targeted UIR.
*   **REQ-MH-GEN-007 (Reflective Inquiry - TID_ASO_META_003):** When an MH elicits input or presents choices, its PUMs/UIRs MUST articulate its current understanding and the rationale for its request.

---

**III. Core Functional Capabilities: Meta-Handlers (Continued)**

*(General MH Requirements REQ-MH-GEN-001 to REQ-MH-GEN-007 from Segment 2 apply to all MHs below)*

*   **REQ-IFE-001 (Idea Formalization Engine - IFE-MH):**
    *   **REQ-IFE-001.1 (Goal):** To take a user's initial idea/problem statement and transform it into a formalized "Project Core Essence" and a structured initial CCO, ready for project definition.
    *   **REQ-IFE-001.2 (s1 - Initiation & Input Elicitation):**
        *   Input: Optional initial user prompt (`mi.user_initial_prompt`).
        *   Action: Initialize `self.eng.sI`. If no initial prompt, use `format_input_elicit_text` to get the core idea.
        *   Output: `A_LLM` (UIR_ET) if eliciting, or internal transition to `ife_s2` with the prompt.
    *   **REQ-IFE-001.3 (s2 - Input Processing & CCO Initialization):**
        *   Input: User's core idea text (from `llr.c`).
        *   Action:
            1.  Store raw input in `self.eng.sI`.
            2.  If `self.eng.cco` is `None`, initialize a new CCO: generate `cco_id`, set basic metadata (`name_label` from input summary, `current_form`="IFE_InputReceived", `schema_version_used`, `engine_version_context`, timestamp, `initiating_document_s` with raw input). Log CCO creation.
            3.  Use `format_process_input_document_request` for LLM analysis (summary, keywords, etc.).
        *   Output: `A_LLM` (CT_PROCESS_INPUT).
    *   **REQ-IFE-001.4 (s2_callback_process_input - Process LLM Analysis & Draft Essence Request):**
        *   Input: LLM response from `CT_PROCESS_INPUT`.
        *   Action:
            1.  Store structured analysis in `self.eng.sI` and update `self.eng.cco.initiating_document_s.processed_representation`.
            2.  Update CCO metadata (`name_label`, `current_state_description`="IFE - Input Processed, Ready for Essence Draft").
            3.  If LLM processing failed, set error status, PUM error, and return to Kernel.
            4.  Use `format_draft_text_segment` to request LLM to draft "Project Core Essence Text" (1-3 sentences) based on processed input, passing `aux_data_refs` to original input.
        *   Output: `A_LLM` (CT_DDT).
    *   **REQ-IFE-001.5 (s3 - Refine Essence via MRO):**
        *   Input: LLM response (drafted core essence) from `CT_DDT`.
        *   Action:
            1.  Store draft essence in `self.eng.sI`. Handle failed draft with placeholder.
            2.  Use `format_mro_request` to submit draft essence for MRO refinement. Guidance: clarity, impact, alignment with processed input. Pass `aux_data_refs`.
        *   Output: `A_LLM` (CT_MRO_REFINE_STUB).
    *   **REQ-IFE-001.6 (s4 - Finalize Essence & Complete IFE):**
        *   Input: MRO refinement result.
        *   Action:
            1.  Process MRO result (use pre-MRO draft or placeholder if MRO failed).
            2.  Update `self.eng.cco.core_essence_j` with the final JSON object (containing essence text and MRO status).
            3.  Update `self.eng.cco.metadata_internal_cco.current_form` to "PDF_Ready".
            4.  Update `self.eng.cco.metadata_internal_cco.current_state_description` to "IFE Complete - Project Essence Defined".
            5.  Log IFE completion to CCO. Clear transient data from `self.eng.sI`.
        *   Output: Return `{"s": "IFE_Complete", "cco_cur_d": self.eng.cco, "dfl": {"summary": "IFE Done for CCO: <cco_id>"}}` to Kernel.

*   **REQ-PDF-001 (Project Definition Finalization - PDF-MH - Functional Implementation of Design):**
    *   **REQ-PDF-001.1 (Goal):** To take a CCO from IFE, validate its core elements, elicit further clarifying details from the user (scope, constraints, deliverables, success criteria), and generate a concise "Project Definition Summary" or "Project Charter," preparing the CCO for planning.
    *   **REQ-PDF-001.2 (s1 - Initialization & Review):**
        *   Input: CCO from IFE.
        *   Action: Initialize `self.eng.sP`. Load and review `cco.core_essence_j` and `cco.initiating_document_s`. Log PDF start.
        *   Output: `A_LLM` (UIR_ET using `format_input_elicit_text`) to prompt user for further details/constraints, or internal transition if no elicitation is deemed necessary by initial review.
    *   **REQ-PDF-001.3 (s2 - Process Clarifications & Prepare for Charter Draft):**
        *   Input: User response from elicitation (if any).
        *   Action: Store user clarifications in `self.eng.sP` and log to CCO (e.g., `cco.initiating_document_s.elicited_pdf_details_json`). Prepare context for charter drafting (core essence, input summary, user clarifications).
        *   Output: `A_LLM` (CT_DDT using `format_draft_text_segment`) to request LLM to draft "Project Definition Summary/Charter" (e.g., 2-5 paragraphs covering problem/goal, scope, key objectives/deliverables, major constraints).
    *   **REQ-PDF-001.4 (s3 - Refine Charter via MRO):**
        *   Input: LLM response (drafted charter) from `CT_DDT`.
        *   Action: Store draft charter in `self.eng.sP`. Use `format_mro_request` to submit for MRO refinement. Guidance: clarity, completeness for project definition, impact.
        *   Output: `A_LLM` (CT_MRO_REFINE_STUB).
    *   **REQ-PDF-001.5 (s4 - Finalize & Update CCO):**
        *   Input: MRO refinement result.
        *   Action: Process refined charter. Update CCO with final refined text in a new field (e.g., `cco.initiating_document_s.project_charter_summary_json`). Update `cco.metadata_internal_cco.current_form` to "PLAN_Ready" and `current_state_description` to "PDF Complete - Project Defined & Chartered". Log PDF completion. Clear `self.eng.sP`.
        *   Output: Return `{"s": "PDF_Complete", "cco_cur_d": self.eng.cco, "dfl": {"summary": "PDF Done for CCO: <cco_id>"}}` to Kernel.

*   **REQ-PLN-001 (Planning - PLAN-MH - Functional Implementation of Design):**
    *   **REQ-PLN-001.1 (Goal):** To take a CCO with a defined project (from PDF) and generate a structured, actionable project plan (phases, tasks, dependencies, etc.) within the CCO.
    *   **REQ-PLN-001.2 (s1 - Initialization & Goal Ingestion):**
        *   Input: CCO from PDF.
        *   Action: Initialize `self.eng.sPl`. Load and analyze `cco.core_essence_j` and `cco.initiating_document_s.project_charter_summary_json` to understand the overall project goal and scope. Log PLAN start.
        *   Output: `A_LLM` (e.g., CT_DECOMPOSE_GOAL_TO_PHASES using a generic `format_draft_text_segment` or a new specialized wrapper) to request LLM to propose major project phases and high-level milestones based on the project definition. Expected output: list of phase objects.
    *   **REQ-PLN-001.3 (s2 - Process Phases & Initiate Task Generation per Phase):**
        *   Input: LLM response (list of phase objects).
        *   Action: Store phases in `self.eng.sPl`. If no phases, PUM error and return. Initialize loop through phases. For the first phase, use `format_draft_text_segment` (or new wrapper `CT_GENERATE_TASKS_FOR_PHASE`) to request LLM to generate detailed tasks (with description, dependencies, effort estimate, suggested MH/CT, optional rhetorical goal).
        *   Output: `A_LLM` (CT_DDT or CT_GENERATE_TASKS_FOR_PHASE) for tasks of the first phase.
    *   **REQ-PLN-001.4 (sX_callback_process_tasks_and_next_phase - Iterative Step):**
        *   Input: LLM response (list of task objects for a phase).
        *   Action: Store tasks for the completed phase in `self.eng.sPl`. Log. If more phases remain, prepare and request tasks for the next phase (looping back to use `format_draft_text_segment` / `CT_GENERATE_TASKS_FOR_PHASE`).
        *   Output: `A_LLM` for next phase's tasks, or internal transition to finalize if all phases done.
    *   **REQ-PLN-001.5 (sY - Finalize Plan & Update CCO):**
        *   Action: Once tasks for all phases are generated, consolidate the full plan (phases and tasks) into a structured JSON object. Update `cco.project_plan_j` (per TID_CCO_SCHEMA_UPDATE_PLANNING_V1). Update `cco.metadata_internal_cco.current_form` to "TDE_Ready" and `current_state_description` to "PLAN Complete - Project Plan Defined". Log PLAN completion. Clear `self.eng.sPl`.
        *   Output: Return `{"s": "PLAN_Complete", "cco_cur_d": self.eng.cco, "dfl": {"summary": "PLAN Done for CCO: <cco_id>"}}` to Kernel.

*   **REQ-TDE-001 (Task Decomposition & Execution - TDE-MH - Functional - TID_TDE_FUNCTIONAL_V1):**
    *   **REQ-TDE-001.1 (Goal):** To execute the project plan stored in `cco.project_plan_j` by iterating through phases and tasks, dispatching tasks to appropriate sub-MHs (CAG, SEL, KAU) or orchestrating direct LLM cognitive tasks, and updating task statuses.
    *   **REQ-TDE-001.2 (s1 - Initialization & Plan Loading):**
        *   Input: CCO from PLAN.
        *   Action: Initialize `self.eng.sT`. Load and validate `cco.project_plan_j`. If no valid plan, PUM error and return. Set current phase/task index to start. Log TDE start.
        *   Output: Internal transition to process the first task.
    *   **REQ-TDE-001.3 (sX_process_next_task - Iterative Step):**
        *   Action:
            1.  Identify next task from `self.eng.sT.plan` based on current indices and dependencies. If no more tasks, transition to finalize.
            2.  PUM info about starting task. Update task status in CCO to "In Progress."
            3.  Examine `task.suggested_mh_or_ct`.
            4.  If sub-MH (e.g., "CAG"): Set `self.eng.kCI` to target MH ID (e.g., "CAG"), `self.eng.kMI` with task details and CCO JSON. Store current TDE state (`self.eng.sT`) and the ID of the task being dispatched. Return to Kernel for sub-MH execution.
            5.  If direct LLM CT: Use appropriate Cognitive Wrapper to formulate and send request.
            6.  If user action required: Use `format_input_elicit_text` or `format_input_prompt_options`.
        *   Output: `A_LLM` (for LLM CT or user input), or return to Kernel for sub-MH dispatch.
    *   **REQ-TDE-001.4 (sY_callback_process_task_result - Iterative Step):**
        *   Input: Result from sub-MH completion (passed by Kernel) or LLM response.
        *   Action:
            1.  Update task status in CCO (e.g., "Completed," "Failed," "Pending_Review") based on result. Store/link task output in CCO (e.g., in `execution_log_detailed_json` or linked from `project_plan_j.tasks[X].output_ref`).
            2.  Log task completion/failure.
            3.  Increment task index.
            4.  If end of phase, PUM phase completion, perform CCO Phase Reset actions (log archive of phase, update `current_phase_id` in CCO metadata).
        *   Output: Internal transition to `sX_process_next_task`.
    *   **REQ-TDE-001.5 (sZ - Finalize Execution):**
        *   Action: When all tasks in all phases are completed, update `cco.metadata_internal_cco.current_form` to "KAU_Ready" (or "ProjectComplete_ReviewPending") and `current_state_description` to "TDE Complete - All Tasks Processed". Log TDE completion. Clear `self.eng.sT`.
        *   Output: Return `{"s": "TDE_AllTasksComplete", "cco_cur_d": self.eng.cco, "dfl": {"summary": "TDE Done for CCO: <cco_id>"}}` to Kernel.

---

**III. Core Functional Capabilities: Meta-Handlers (Continued)**

*(General MH Requirements REQ-MH-GEN-001 to REQ-MH-GEN-007 from Segment 2 apply to all MHs below)*

*   **REQ-CAG-001 (Collaborative Artifact Generation - CAG-MH - TID_ASO_META_001, TID_ASO_META_006):**
    *   **REQ-CAG-001.1 (Goal):** To generate specified content segments (e.g., document sections, code modules) based on CCO context, task instructions from `project_plan_j` (including rhetorical goals), and to iteratively refine these segments with MRO and user feedback.
    *   **REQ-CAG-001.2 (s1 - Initialization & Scoped Planning):**
        *   Input: CCO, `task_details` (from TDE, including `target_segment_identifier`, `target_segment_goal`, `target_segment_rhetorical_goal`, `relevant_context_refs_from_cco`).
        *   Action: Initialize `self.eng.sCg`. Log CAG start. Perform "Proactive CCO Theme Integration Review": explicitly review CCO (`core_essence_j`, `knowledge_artifacts_contextual_json.conceptual_anchors_cco_json`, `op_log_j` for insights) to identify unique project themes/anchors relevant to the current segment. Store these in `self.eng.sCg.identified_anchors`.
        *   Output: `A_LLM` (CT_DDT using `format_draft_text_segment`) to request LLM to draft the initial content segment. The prompt MUST include the identified anchors, rhetorical goal, and other context from `task_details`.
    *   **REQ-CAG-001.3 (s2 - Process Initial Draft & Initiate MRO):**
        *   Input: LLM response (drafted segment) from `CT_DDT`.
        *   Action: Store draft in `self.eng.sCg.current_draft_json`. If draft failed, use placeholder and log error. Initiate MRO refinement using `format_mro_request`. MRO goals MUST include adherence to rhetorical goal, integration of identified anchors, plus standard quality criteria (Novelty, Transformative Value, Impact, Info Density per REQ-MRO-001).
        *   Output: `A_LLM` (CT_MRO_REFINE_STUB).
    *   **REQ-CAG-001.4 (s3_callback_process_mro_and_user_review - Iterative Step):**
        *   Input: MRO refinement result.
        *   Action:
            1.  Process MRO result. Update `self.eng.sCg.current_draft_json`.
            2.  Present the refined draft to the user for review via PUM (including a summary of MRO changes if available from MRO output).
            3.  Request user feedback/approval via `format_input_elicit_text` (e.g., "Draft for [segment] refined. Please review and provide feedback or type 'APPROVE'.").
        *   Output: `A_LLM` (UIR_ET).
    *   **REQ-CAG-001.5 (s4_callback_process_user_feedback_and_iterate_or_finalize - Iterative Step):**
        *   Input: User feedback/approval.
        *   Action:
            1.  If user provides feedback for revision: Store feedback. Log it. Update `self.eng.sCg.identified_anchors` or refinement goals if feedback implies changes. Loop back to `s2` (or a dedicated MRO re-initiation step within CAG) to re-refine based on new feedback. Increment iteration counter in `self.eng.sCg`.
            2.  If user approves: Proceed to finalize.
            3.  If user command intercepted: Return to Kernel.
    *   **REQ-CAG-001.6 (s5 - Finalize Segment):**
        *   Action: Store final approved draft in the CCO at the location specified by `task_details.output_cco_path` (e.g., within `product_content_data_json`). Update task status in `project_plan_j` to "Completed." Log CAG completion for this segment. Clear relevant parts of `self.eng.sCg`.
        *   Output: Return `{"s": "CAG_SegmentComplete", "cco_cur_d": self.eng.cco, "dfl": {"summary": "CAG completed segment: <segment_id>"}}` to Kernel (or to TDE if called as sub-MH).
    *   **REQ-CAG-001.7 (Draft Management - TID_ASO_META_006):** CAG-MH MUST internally track its immediately preceding draft version for a given segment within a single task execution instance to support potential user requests for comparison or reversion during the iterative feedback loop (s4).

*   **REQ-MRO-001 (Meta-Refine Output - MRO-MH - TID_ASO_META_002, TID_ASO_META_005, FIR_009):**
    *   **REQ-MRO-001.1 (Goal):** To iteratively refine AI-generated content (passed as JSON string) based on specified goals, quality criteria, and CCO context, aiming for high transformative value, information density, and adherence to conceptual anchors.
    *   **REQ-MRO-001.2 (s1_initialize - `mro_s1i`):**
        *   Input: `DraftOutput_JsonString`, `RefinementGoals_JsonString`, `CCOContext_JsonString`, `caller_mh_context`, `caller_continuation_hint`, `MaxIterations`, `IsFrameworkComponent`, `PreviousDraftOutput_JsonString_Optional`.
        *   Action: Initialize `self.eng.sMr` with inputs. Parse `RefinementGoals_JsonString`. Log MRO start.
        *   Output: Internal transition to `_mro_loop_step`.
    *   **REQ-MRO-001.3 (`_mro_loop_step` - Iterative Core):**
        *   Action: If `CurrentIteration >= MaxIterations` or `ConvergenceMet`, transition to `_mro_finalize`. Increment iteration. PUM status.
        *   Use `format_cognitive_request` (or a new specialized wrapper `format_mro_critique_request`) to ask LLM to:
            1.  Query adaptive critique rules (based on CCO LHR/LHL, global heuristics, and `critique_focus_hint` from refinement goals).
            2.  Perform content critique against `quality_criteria_json` (including Novelty, Transformative Value, Impact, Info Density, Conciseness, Anchor Integration) and adaptive rules.
            3.  If `IsFrameworkComponent` or `requires_schema_validation`, validate against `target_schema_name`.
            4.  Synthesize all critique into an actionable summary (identifying if thresholds met, number of issues).
        *   Output: `A_LLM` requests for these chained or batched cognitive tasks.
    *   **REQ-MRO-001.4 (Callbacks for Critique & Revision - `mro_cacr` through `mro_cacv`):**
        *   Action: Process LLM responses for critique rules, content critique, schema validation, and critique synthesis.
        *   If critique synthesis indicates convergence (thresholds met, no actionable issues), set `ConvergenceMet = True` and loop.
        *   Else, use `format_cognitive_request` (or new wrapper `format_mro_revision_request`) to ask LLM to:
            1.  Suggest specific revisions based on synthesized critique.
            2.  If actionable suggestions exist, apply revisions to current draft.
            3.  Compare revised draft with previous draft (using `analysis_compare_content_versions_v3` logic) to check for significant improvement. If not significant, set `ConvergenceMet = True`.
        *   Update `self.eng.sMr.CurrentDraft_JsonString`. Loop back to `_mro_loop_step`.
    *   **REQ-MRO-001.5 (`_mro_finalize` - Completion):**
        *   Action: Log MRO completion.
        *   Output: Return `{"s": "MRO_PIPELINE_COMPLETE", "mro_result": {"refined_output_json": ..., "refinement_summary_json": ..., "status": ...}, "caller_mh_context": ..., "caller_continuation_hint": ..., "cco_cur_d": self.eng.cco}` to Kernel.
    *   **REQ-MRO-001.6 (Performance):** If quantitative proxies indicate diminishing returns over N (e.g., 2) iterations, MRO should favor convergence or flag for user review.

*   **REQ-SEL-001 (Solution/Style Exploration & Learning - SEL-MH - Functional):**
    *   **REQ-SEL-001.1 (Goal):** To generate and evaluate potential solutions for a defined problem, or to learn and apply stylistic/structural preferences.
    *   **REQ-SEL-001.2 (s1 - Initialization & Mode Determination):**
        *   Input: CCO, `task_details` (specifying problem context or style learning task).
        *   Action: Initialize `self.eng.sSl`. Determine mode (solution exploration vs. style learning). Log SEL start.
        *   If solution exploration: Use `format_cognitive_request` (or new `format_sel_generate_options_request`) to ask LLM to generate multiple solution options based on `problem_context` from CCO.
        *   If style learning: (Future) Prompt user for example content or style directives.
        *   Output: `A_LLM` for option generation or user input.
    *   **REQ-SEL-001.3 (s2 - Process Options & Initiate Evaluation - Solution Mode):**
        *   Input: LLM response (list of solution options).
        *   Action: Store options in `self.eng.sSl`. If no options, PUM error. Define/elicit evaluation criteria. Loop through options: for each, use `format_cognitive_request` (or new `format_sel_evaluate_option_request`) to ask LLM to evaluate it against criteria.
        *   Output: `A_LLM` for first option evaluation.
    *   **REQ-SEL-001.4 (sX_callback_process_evaluation_and_next - Iterative Step):**
        *   Input: LLM response (evaluation of one option).
        *   Action: Store evaluation. If more options, request evaluation for next.
        *   Output: `A_LLM` for next evaluation, or internal transition if all done.
    *   **REQ-SEL-001.5 (sY - Present Evaluated Options & Get User Choice):**
        *   Action: Once all options evaluated, use `format_input_prompt_options` to present summarized options and evaluations to user for selection.
        *   Output: `A_LLM` (UIR_PO).
    *   **REQ-SEL-001.6 (sZ - Finalize Selection):**
        *   Input: User's selected option.
        *   Action: Store chosen solution (with its evaluation) in CCO (e.g., `knowledge_artifacts_contextual_json.selected_solution_details_json`). Log SEL completion. Clear `self.eng.sSl`.
        *   Output: Return `{"s": "SEL_Complete", "cco_cur_d": self.eng.cco, "dfl": ...}` to Kernel/TDE.

*   **REQ-KAU-001 (Knowledge Artifact Update - KAU-MH - Functional - REQ-KAU-002, REQ-KAU-003):**
    *   **REQ-KAU-001.1 (Goal):** To extract learnings (LHRs/LHLs) from CCO operational history, user feedback, or MRO summaries, and update relevant KAs in the CCO. (Future: manage global heuristic promotion, insight generation).
    *   **REQ-KAU-001.2 (s1 - Initialization & Data Scoping):**
        *   Input: CCO, `task_details` (specifying data source for learning, e.g., "current_cco_op_log", "user_feedback_segment_X", or "all_project_data_for_post_mortem").
        *   Action: Initialize `self.eng.sK`. Identify and prepare data for analysis. Log KAU start.
        *   Output: `A_LLM` (using `format_cognitive_request` or new `format_kau_extract_learnings_request`) to ask LLM to analyze the scoped data and extract potential LHRs/LHLs (structured as objects with description, context, type, confidence, source).
    *   **REQ-KAU-001.3 (s2 - Process & Store Learnings):**
        *   Input: LLM response (list of potential LHR/LHL objects).
        *   Action: Filter/validate learnings. Update `cco.knowledge_artifacts_contextual_json` (LHR and LHL lists). Log KAU completion. Clear `self.eng.sK`.
        *   Output: Return `{"s": "KAU_Complete", "cco_cur_d": self.eng.cco, "dfl": {"learnings_added": <count>}}` to Kernel.
    *   **REQ-KAU-001.4 (Insight Generation - REQ-KAU-002 - Future):** KAU-MH to attempt to identify correlations across LHLs/op_log_j to flag potential systemic insights or areas for framework improvement (generating a proto-TID).
    *   **REQ-KAU-001.5 (Global Heuristic Synthesis - REQ-KAU-003 - Future):** KAU-MH to manage a process for reviewing, validating, and proposing CCO-specific LHRs for promotion to a Global Heuristic Repository.

*   **REQ-FEL-001 (Framework Evolution Loop - FEL-MH - REQ-FEL-001.1, REQ-FEL-002, REQ-FEL-003):**
    *   **REQ-FEL-001.1 (Goal):** To manage the evolution of the AIOS Engine script itself by processing TIDs, orchestrating design/code generation tasks, and outputting a new, validated engine script version.
    *   **REQ-FEL-001.2 (s1 - Initialization & Goal/TID Ingestion):**
        *   Input: Optional `evolution_goal` or `tid_source_reference`.
        *   Action: Initialize `self.eng.sFe`. If no goal/TID source, use `format_input_elicit_text` to get evolution goal or TID reference from user. If TID reference, use cognitive task to load/parse TIDs.
        *   Output: `A_LLM` (UIR_ET or CT for TID loading).
    *   **REQ-FEL-001.3 (s2 - Evolution Planning / Design Generation):**
        *   Input: Evolution goal and/or loaded TIDs.
        *   Action: Use `format_plan_aios_evolution_request` to ask LLM to generate a detailed evolution plan. If goal is specific design (e.g., "Design PDF-MH"), this plan *is* the design document. If goal is code change, plan includes conceptual code modifications. Plan must include new TIDs for implementation steps.
        *   Output: `A_LLM` (CT_PLAN_AIOS_EVOLUTION).
    *   **REQ-FEL-001.4 (s3 - Process Plan & Store in CCO):**
        *   Input: LLM response (evolution plan/design document).
        *   Action: Store plan/design in `self.eng.cco.evolution_lab_outputs`. Update CCO metadata. Log FEL planning complete.
        *   Output: If plan involves direct code generation for *this FEL cycle*: internal transition to code generation step. Else (if plan is for future TIDs): Return `{"s": "FEL_Plan_Complete", ...}` to Kernel.
    *   **REQ-FEL-001.5 (sX_code_generation - Conditional Step):**
        *   Input: Design document / conceptual code changes from previous FEL step.
        *   Action: Orchestrate LLM (via `format_draft_text_segment` or specialized code generation wrapper) to generate the *full text* of the new/modified AIOS Engine Python script. This generation MUST adhere to minification guidelines (REQ-PERF-003) and embed LOHs in "instructions_for_ai" (REQ-OP-003.4).
        *   Output: `A_LLM` (CT_DDT or CT_GENERATE_CODE).
    *   **REQ-FEL-001.6 (sY_validate_and_output_new_engine - Conditional Step):**
        *   Input: Generated new engine script text.
        *   Action:
            1.  Perform conceptual validation (e.g., LLM task to check against schema, LOHs, TID requirements).
            2.  (Future - REQ-TEST-004) Orchestrate conceptual regression testing.
            3.  If validation passes, prepare output package.
        *   Output: Return `{"s": "FEL_EvolutionProposed_Complete", "new_engine_script_text": "...", "changelog": "...", "cco_cur_d": self.eng.cco, "dfl": ...}` to Kernel. The orchestrator is then responsible for saving/applying this new script.
    *   **REQ-FEL-001.7 (Safety Protocols - REQ-FEL-003):** Generated code must undergo conceptual review/sandboxed testing before proposal for live execution.

---

**IV. Central Conceptual Object (CCO) & Data Management**

This section details requirements for the CCO's structure, integrity, and how AIOS interacts with it, emphasizing performance and future scalability.

*   **REQ-CCO-001 (Schema Adherence & Evolution):**
    *   **REQ-CCO-001.1 (Schema Conformance):** The structure of the active CCO (`self.eng.cco`, a Python dictionary) MUST strictly conform to the currently defined `AIOS_CCO_Schema` (e.g., `v3.0.1-mod5` as a baseline, evolving with new functionalities). The schema itself should be conceptually accessible or defined within the AIOS Engine's documentation or as a loadable artifact.
    *   **REQ-CCO-001.2 (Schema Evolution Management - TID_CCO_SCHEMA_UPDATE_PLANNING_V1, FIR_META_002, FIR_001):**
        *   FEL-MH MUST be responsible for managing and versioning the CCO schema definition as part of engine evolution.
        *   The CCO schema MUST evolve to support new MH functionalities and data requirements (e.g., a structured `project_plan_j` for PLN-MH outputs, dedicated fields for PDF-MH outputs like `project_charter_summary_json`, `elicited_pdf_details_json`, and `selected_solution_details_json` for SEL-MH).
        *   All CCO sub-objects (e.g., task definitions, log entries, LHR/LHL entries) must have their schemas explicitly detailed.
    *   **REQ-CCO-001.3 (Schema Version Management & Migration - Perspective 1 Critique):**
        *   The AIOS Engine (`import_state` or an initial CCO load function) MUST detect the schema version of an incoming CCO.
        *   It MUST provide a mechanism or clear protocol for handling CCOs created with older schema versions. Options include:
            *   Attempting automated migration to the current schema version (if feasible and defined by FEL-MH during a schema evolution).
            *   Gracefully operating with older schema versions with clear warnings about potential limitations.
            *   Refusing to load incompatible schemas with an informative error message.
        *   The `metadata_internal_cco.schema_version_used` field is critical for this.

*   **REQ-CCO-002 (Comprehensive & Structured Operational Log - `op_log_j`):**
    *   **REQ-CCO-002.1 (Content):** The `op_log_j` MUST record all significant engine operations, including Kernel decisions, MH invocations and completions (with status), LLM cognitive task requests (summary/type) and completions (summary/status), critical CCO modifications, and user interactions/commands.
    *   **REQ-CCO-002.2 (Structure):** Each log entry MUST be a structured JSON object containing at least: `ts` (timestamp), `let` (log entry type, e.g., "IFE_NEW_CCO_PROCESSED", "MH_COMP_STAT_RPT"), `lm` (log message), `mh_c` (MH context at time of logging), and optional `ado` (associated data object, JSON string).
    *   **REQ-CCO-002.3 (Performance - REQ-PERF-005.3):** Appending to `op_log_j` (via `lco`) must be efficient. For very long-running projects, strategies for managing log size within the `ces` (e.g., periodic summarization into a separate log archive field, or offloading older entries if an external store becomes viable) must be considered by KAU-MH or FEL-MH for future performance optimization.

*   **REQ-CCO-003 (Knowledge Artifacts - `knowledge_artifacts_contextual_json`):**
    *   **REQ-CCO-003.1 (Storage):** This field MUST store various project-specific KAs as JSON strings or structured objects, including (but not limited to): LHRs, LHLs, conceptual anchors, project definitions, selected solutions, style guides, and glossaries.
    *   **REQ-CCO-003.2 (Accessibility):** MHs must be ables to efficiently read from and write to these KAs.

*   **REQ-CCO-004 (Evolution Lab Outputs - `evolution_lab_outputs`):**
    *   **REQ-CCO-004.1 (Storage):** This list in the CCO MUST store outputs from FEL-MH cycles, including the evolution goal, plan status, and detailed `plan_details` (summary, proposed TIDs, discussion, design documents).

*   **REQ-CCO-005 (Data Access & Update Efficiency - REQ-PERF-005.1, REQ-PERF-005.2):**
    *   **REQ-CCO-005.1:** The primary CCO update utility (`f_ucs`) MUST efficiently and safely modify nested data within the `self.eng.cco` Python dictionary.
    *   **REQ-CCO-005.2:** JSON parsing (`pco`) and serialization (`cjo`) of CCO fields must be optimized.

*   **REQ-CCO-006 (Knowledge Graph Potential - Perspective 2 Critique):**
    *   **REQ-CCO-006.1 (Future Design Consideration):** The CCO schema and KAU-MH functionalities should be designed with future potential for extracting or representing knowledge within the CCO as a graph-like structure. This would enable advanced querying and inference about relationships between diverse CCO elements (e.g., how a specific LHL relates to a task in the plan that led to a particular design decision stored in `product_content_data_json`). This is a long-term goal for enhancing AIOS's reasoning capabilities.

**V. User Interaction & Experience (Minimized but High-Quality Role)**

*   **REQ-UIX-001 (Clarity & Conciseness of Interaction):** All PUMs and prompts for user input (UIRs) generated by AIOS MUST be clear, concise, contextually relevant, and unambiguously state what is needed from the user or what action AIOS is taking/proposing.
*   **REQ-UIX-002 (Decision Support Summaries - Perspective 2 Critique):**
    *   **REQ-UIX-002.1:** When AIOS requires user input for strategic decisions (e.g., selecting an evolution goal for FEL, approving a project plan from PLN) or subjective evaluations (e.g., choosing between solution options from SEL, approving a CAG draft after MRO), it MUST provide a well-structured summary.
    *   **REQ-UIX-002.2 (Output Specification - Decision Support):** This summary MUST include: the decision needed, 2-4 concise options if applicable, key pros/cons or implications for each option, and references to relevant CCO data supporting the summary. This may itself be a `CT_SUMMARIZE_FOR_DECISION` cognitive task.
*   **REQ-UIX-003 (Transparency of Reasoning - Perspective 2 Critique, TID_ASO_META_003):**
    *   **REQ-UIX-003.1 (Reflective Inquiry):** AIOS (Kernel & MHs) MUST articulate its interpretation of user intent (distinguishing conceptual vs. literal if necessary) before acting on significant or ambiguous directives.
    *   **REQ-UIX-003.2 (Traceability):** The CCO's `op_log_j` and the sequence of PUMs should allow a knowledgeable user to trace the high-level reasoning or MH path that led to a particular state or request.
*   **REQ-UIX-004 (Minimized Intervention & Confidence-Gated Prompts - REQ-MH-GEN-006.2):**
    *   **REQ-UIX-004.1:** AIOS MHs MUST maximize autonomy in their procedural execution.
    *   **REQ-UIX-004.2:** MHs MUST implement internal confidence assessment for autonomously generated outputs or decisions. If confidence is below a defined (potentially configurable) threshold, the MH must not proceed with high-impact actions but instead flag the item for user review via a PUM, clearly stating the specific concerns or areas of low confidence.
*   **REQ-PHIL-001 (Metacognitive Feedback to User - Perspective 2 Critique):**
    *   **REQ-PHIL-001.1 (Optional Feature):** (Future) KAU-MH, with explicit user opt-in, may analyze user interaction patterns (from `op_log_j` or feedback logs) to provide summarized, constructive metacognitive feedback to the user about their collaboration style with AIOS, aiming to enhance mutual understanding and co-evolutionary efficiency.

**VI. Testing, Validation, and Quality Assurance**

*   **REQ-TEST-001 (Unit Testability Design):** Core Engine Python functions and methods within the Kernel and MHs SHOULD be designed with clear inputs, outputs, and minimal side effects (where possible) to facilitate conceptual unit testability by an LLM or human developer.
*   **REQ-TEST-002 (Integration Test Scenarios):** A suite of conceptual integration test scenarios MUST be defined to validate key MH-to-MH handovers and Kernel-MH interactions (e.g., IFE output correctly consumed by PDF, PDF by PLN, PLN plan correctly interpreted by TDE, TDE dispatch to CAG and processing its return).
*   **REQ-TEST-003 (End-to-End Functional Scenarios):** A defined set of representative end-to-end project scenarios (e.g., "draft a three-section report on topic X from initial idea to final plan," "evolve engine to add new utility function Y") MUST be used to validate overall AIOS functionality.
    *   **REQ-TEST-003.1 (Output Specification - E2E Tests):** Successful completion of an E2E scenario MUST result in a CCO whose final state and key artifacts (e.g., project plan, drafted content, evolution plan) meet predefined quality, completeness, and structural criteria for that scenario.
*   **REQ-TEST-004 (Regression Testing for FEL-MH):**
    *   **REQ-TEST-004.1:** Before FEL-MH finalizes a new engine version proposal, a conceptual regression test suite (based on REQ-TEST-003 scenarios and key unit/integration tests) MUST be "executed" against the conceptual changes. This involves the LLM Orchestrator analyzing the proposed code modifications against the logic of the test cases.
    *   **REQ-TEST-004.2:** The results of this conceptual regression testing (pass/fail, identified issues) MUST be part of the FEL output package and logged in the CCO.
    *   **REQ-TEST-004.3 (Performance Regression):** Regression tests MUST include performance benchmark comparisons (REQ-TEST-005) to ensure evolved engines do not unacceptably degrade performance.
*   **REQ-TEST-005 (Performance Benchmarking & KPIs):**
    *   **REQ-TEST-005.1:** Establish and maintain a suite of benchmark scenarios and CCOs to measure Key Performance Indicators (KPIs) including:
        *   `ces` export/import time (REQ-PERF-002).
        *   Engine script parse time (REQ-PERF-003).
        *   Typical MH step Python execution time (excluding LLM calls) (REQ-PERF-001).
    *   **REQ-TEST-005.2:** These KPIs MUST be tracked across engine versions to monitor and validate performance impacts of evolution.
*   **REQ-TEST-006 (Minification Validation - REQ-PERF-003.4):** Minified engine versions produced by FEL-MH MUST pass all applicable functional (REQ-TEST-003) and performance (REQ-TEST-005) benchmarks established for their conceptual verbose counterparts or previous minified versions.

**VII. Configuration Management (NEW SECTION - Perspective 1 Self-Critique)**

*   **REQ-CONF-001 (Engine Configuration Parameters):** The AIOS Engine MUST support a mechanism for managing key operational parameters.
    *   **REQ-CONF-001.1 (Parameter Examples):** Configurable parameters should include (but not be limited to): default MRO iteration counts, confidence thresholds for user prompting (REQ-UIX-004.2), logging verbosity levels, specific LLM model preferences for different cognitive task types (if supported by Cognitive Wrappers - REQ-ARC-001).
    *   **REQ-CONF-001.2 (Management Mechanism):** These parameters MAY be:
        *   Set at engine instantiation via arguments to `__init__`.
        *   Loaded from a dedicated configuration object/JSON string passed during instantiation.
        *   (Future) Modifiable at runtime via a specific system command and stored within the `ces`.
    *   **REQ-CONF-001.3 (Default Values):** The engine MUST have sensible default values for all configuration parameters.
    *   **REQ-CONF-001.4 (Documentation):** All configurable parameters and their effects MUST be documented in the AIOS User Manual.

---

